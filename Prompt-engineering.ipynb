{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1d3ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MODEL = \"llama3\"\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c4f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What an honor!\n",
      "\n",
      "I'd be delighted to help you craft a heartfelt birthday card for your dear friend Samar. Here's a draft message to get us started:\n",
      "\n",
      "\"Dear Samar,\n",
      "\n",
      "Happy birthday to an incredible friend who brings so much joy and love into my life! As we celebrate another year of life, I want to express how grateful I am for our friendship. Your kind heart, infectious laugh, and unwavering support mean the world to me.\n",
      "\n",
      "Here's to many more adventures, late-night conversations, and memories to be made together! Wishing you a birthday as bright and beautiful as you are.\n",
      "\n",
      "With love and appreciation,\n",
      "[Your Name]\"\n",
      "\n",
      "Feel free to modify it as you see fit to make it your own. You could also add inside jokes, personal anecdotes, or special memories that are unique to your friendship with Samar.\n",
      "\n",
      "What do you think? Would you like me to suggest any changes or additions?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Help me write a birthday card for my dear friend Samar.\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7490d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a wonderful gesture!\n",
      "\n",
      "Here's a draft birthday card message for your dear friend Samar:\n",
      "\n",
      "\"Happy Birthday, Samar!\n",
      "\n",
      "As you celebrate another year of life, I want to take a moment to express how grateful I am to have you in my life. You're more than just a friend - you're a partner in crime, a confidant, and a constant source of joy and support.\n",
      "\n",
      "Here's to many more adventures, laughter-filled moments, and late-night conversations about life, love,\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)\n",
    "prompt = \"Help me write a birthday card for my dear friend Samar.\"\n",
    "response = model.invoke(prompt, temperature=0.0, num_predict=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c37168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] \n",
      "    What are fun activities I can do this weekend?\n",
      " [/INST]\n",
      "I'd be happy to help you come up with some fun activity ideas for the weekend!\n",
      "\n",
      "Here are a few suggestions based on different interests:\n",
      "\n",
      "**Outdoor enthusiasts:**\n",
      "\n",
      "1. Go for a hike or nature walk in a nearby park or nature reserve.\n",
      "2. Plan a camping trip or a day-long picnic with friends.\n",
      "3. Try kayaking, paddle-boarding, or renting a bike and exploring local trails.\n",
      "\n",
      "**Indoor fun:**\n",
      "\n",
      "1. Host a game night with friends (board games, card games, or video games).\n",
      "2. Have a movie marathon featuring your favorite films or TV shows.\n",
      "3. Practice yoga or meditation, or try out a new workout routine.\n",
      "\n",
      "**Creative types:**\n",
      "\n",
      "1. Paint or draw with watercolors, acrylics, or digital tools.\n",
      "2. Write poetry, short stories, or even start a journaling practice.\n",
      "3. Take up photography and capture the beauty around you.\n",
      "\n",
      "**Foodies and drink enthusiasts:**\n",
      "\n",
      "1. Try out a new\n",
      "</s>\n",
      "<s>[INST] \n",
      "Which of these would be good for my health?\n",
      " [/INST]\n",
      "\n",
      "As a conversational AI, I'm happy to help you explore fun activities that can also benefit your physical and mental well-being! \n",
      "\n",
      "From the previous suggestions, here are some that can positively impact your health:\n",
      "\n",
      "* **Outdoor enthusiasts:**\n",
      "\t+ Going for a hike or nature walk (great exercise and vitamin D boost!)\n",
      "\t+ Camping trip or picnic with friends (social connection and outdoor fresh air)\n",
      "\t+ Kayaking, paddle-boarding, or bike riding (physical activity and potential stress relief)\n",
      "* **Indoor fun:**\n",
      "\t+ Yoga or meditation practice (stress reduction, flexibility, and mindfulness)\n",
      "\t+ Writing or journaling (creative expression and emotional release)\n",
      "\n",
      "These activities can help:\n",
      "\n",
      "1. Boost mood and reduce stress\n",
      "2. Increase physical activity levels\n",
      "3. Improve mental clarity and focus\n",
      "4. Enhance social connections and sense of community\n",
      "\n",
      "Remember to listen to your body and prioritize self-care. If you have specific health concerns or limitations, consider modifying these activities or consulting with a healthcare professional.\n",
      "\n",
      "Which one of these activities resonates with you the most?\n"
     ]
    }
   ],
   "source": [
    "######Multi-trun Conversations#############\n",
    "\n",
    "prompt_1 = \"\"\"\n",
    "    What are fun activities I can do this weekend?\n",
    "\"\"\"\n",
    "response_1 = model.invoke(prompt_1, num_predict=200)\n",
    "\n",
    "prompt_2 = \"\"\"\n",
    "Which of these would be good for my health?\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = f\"\"\"\n",
    "<s>[INST] {prompt_1} [/INST]\n",
    "{response_1}\n",
    "</s>\n",
    "<s>[INST] {prompt_2} [/INST]\n",
    "\"\"\"\n",
    "print(chat_prompt)\n",
    "\n",
    "response_2 = model.invoke(chat_prompt)\n",
    "\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d1716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this text is POSITIVE. The words \"thoughtful\" and \"thanks\" convey a sense of gratitude and appreciation, indicating that the recipient values the gesture and is pleased with the birthday card.\n"
     ]
    }
   ],
   "source": [
    "##Zero-shot prompting\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is the sentiment of:\n",
    "Hi Samar, thanks for the thoughtful birthday card!\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f905d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE\n",
      "\n",
      "The message expresses gratitude and appreciation for a thoughtful gesture (the birthday card), which indicates a positive sentiment.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Samar, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc7ced66-2026-4b87-888d-6a98ea07fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would guess that the sentiment of the second message is Negative. The fact that the person is mentioning being \"late\" implies a sense of frustration or disappointment, and the tone seems more critical than celebratory.\n"
     ]
    }
   ],
   "source": [
    "##One-shot prompting\n",
    "\n",
    "prompt = \"\"\"\n",
    "Message: Hi Samar, thanks for the thoughtful birthday card!\n",
    "Sentiment: Positive\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d308c7-5547-441c-ad7e-c16b5a68c16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question mark sentiment!\n",
      "\n",
      "In this case, I would classify the sentiment as \"Positive\" since the message is thanking someone (Samar) for a thoughtful act (a birthday card), which implies a positive and appreciative tone.\n"
     ]
    }
   ],
   "source": [
    "##Few-shot prompting\n",
    "\n",
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Samar, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf0ded0-ce24-400f-94e6-afa8566cd0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "### Specifying the Output Format\n",
    "\n",
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Samar, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response either Positive or Negative or Neutral.\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c10c07d-3e54-4a1e-91cf-edf33e5dde05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a profound and age-old question!\n",
      "\n",
      "There is no one definitive answer to this question, as it's deeply personal and subjective. However, here are some possible approaches to answering your friend:\n",
      "\n",
      "1. **Philosophical perspective**: Life has inherent meaning because we, as human beings, have the capacity for consciousness, self-awareness, and the ability to create our own purpose. We can choose how we want to live, what values we want to uphold, and what goals we want to achieve.\n",
      "2. **Existentialist view**: Life's meaning is created by individuals through their experiences, choices, and relationships. It's a personal, subjective reality that each person must forge for themselves. This perspective emphasizes individual freedom and responsibility in shaping one's own life's meaning.\n",
      "3. **Scientific perspective**: From a scientific standpoint, the meaning of life might be understood as a product of evolutionary pressures. Our existence is shaped by natural selection, which favors individuals who contribute to their species' survival and reproduction. In this sense, life has an inherent meaning in terms of perpetuating our genetic legacy.\n",
      "4. **Spiritual or religious perspective**: For many people, the meaning of life is rooted in their faith or spiritual beliefs. They may find purpose and significance through their connection with a higher power, a moral framework, or a sense of divine purpose.\n",
      "5. **Practical approach**: If your friend is looking for more concrete answers, you could discuss some common themes that give people's lives meaning, such as:\n",
      "\t* Pursuing passions and interests\n",
      "\t* Building strong relationships and connections with others\n",
      "\t* Contributing to one's community or society\n",
      "\t* Achieving personal growth and self-improvement\n",
      "\t* Leaving a positive impact on the world\n",
      "\n",
      "When responding to your friend, you might consider sharing your own thoughts and experiences about what gives life meaning. This could be a great opportunity for a meaningful conversation!\n",
      "\n",
      "Remember that there is no one \"right\" answer to this question. The meaning of life is unique to each individual, and it's up to us to create our own purpose and significance.\n"
     ]
    }
   ],
   "source": [
    "### Role Prompting\n",
    "\n",
    "prompt = \"\"\"\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "202dfe98-148c-4f32-83df-29063069eb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, me hearty! Ye be askin' yerself what be the meaning o' life, eh? Well, matey, that be a question that's puzzled landlubbers and sea dogs alike for centuries!\n",
      "\n",
      "Now, I ain't one fer gettin' all bogged down in philosophy and whatnot, but I'll give ye me two cents. Ye see, findin' the meaning o' life be like chartin' a course through treacherous waters. It takes time, patience, and a willingness to take risks.\n",
      "\n",
      "First off, let's start with the basics. What makes ye happy, matey? Is it spendin' time with loved ones, pursuein' yer passions, or just enjoying a good swashbucklin' adventure? Whatever it be, that be a good place to start lookin' for meaning.\n",
      "\n",
      "Next, consider yer values and principles. What be important to ye? Do ye believe in helpin' others, standin' up fer yerself, or just livin' life on yer own terms? When ye align yer actions with yer values, that be a big part o' findin' the meaning o' life.\n",
      "\n",
      "And don't ferget about growin', matey! Life be like a ship at sea - it's always changin' and requiretin' adjustments. Be willin' to learn from yer mistakes, take on new challenges, and adapt to changing circumstances.\n",
      "\n",
      "So, me hearty, here be some advice: Don't be afraid to explore, question, and challenge yerself. And remember, the meaning o' life ain't just about findin' treasure or reachin' a certain destination - it's about livin' in the moment, enjoyin' the journey, and makin' the most o' the time ye have on this here earth!\n",
      "\n",
      "Now, go forth and chart yer own course, matey! And don't ferget to keep yer wits about ye and watch out for them scurvy dogs who might be tryin' to steal yer treasure... er, I mean, yer sense o' purpose! Arrrr!\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"\n",
    "Your role is a life coach \\\n",
    "who gives advice to people about living a good life.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the tone of an English pirate.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2e1a021-4faa-4868-8998-270c92602838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the email:\n",
      "\n",
      "The email discusses the increasing availability of large language models (LLMs) with permissive licenses, making it easier for developers to build applications. The author, Andrew Ng, outlines four ways to build applications based on LLMs: prompting, one-shot or few-shot prompting, fine-tuning, and pretraining. He recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "\n",
      "Key points:\n",
      "\n",
      "* Four ways to build applications based on LLMs:\n",
      "\t+ Prompting: giving a pretrained LLM instructions\n",
      "\t+ One-shot or few-shot prompting: adding examples of desired output\n",
      "\t+ Fine-tuning: training an LLM further on a small dataset\n",
      "\t+ Pretraining: creating your own LLM from scratch\n",
      "* Start with prompting and gradually move to more complex techniques if needed\n",
      "* Choose a specific model based on the application's requirements (smaller models for smaller applications, larger models for more complex reasoning)\n",
      "* The author recommends taking an in-depth course on Generative AI with Large Language Models to learn more about these options.\n",
      "\n",
      "As for what Andrew Ng said about llama models:\n",
      "\n",
      "* He mentions Llama-2-7B as a model that one of his team members is trying to fine-tune to sound like him, implying that it's a large language model. However, he doesn't provide any specific insights or recommendations on using Llama-2-7B or other llama models for building applications.\n"
     ]
    }
   ],
   "source": [
    "### Summarization\n",
    "\n",
    "email = \"\"\"\n",
    "Dear Samar,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task ‚Äî the input and the desired output ‚Äî sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you‚Äôre unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn‚Äôt work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn‚Äôt deliver the performance you want, then try fine-tuning ‚Äî but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? üòú)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that‚Äôs not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM‚Äôs output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning ‚Äî in which GPT-4 surpasses current open models ‚Äî it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, it‚Äôs also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I‚Äôll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew NG\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "What did the author say about llama models?:\n",
    "\n",
    "email: {email}\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "029d5cb8-a8e3-4dc4-b779-c3893204b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2023 Women's World Cup has not yet taken place. The most recent edition of the tournament was the 2019 FIFA Women's World Cup, which was won by the United States women's national team. They defeated the Netherlands 2-0 in the final on July 7, 2019.\n",
      "\n",
      "It's likely that the 2023 Women's World Cup will be held in 2023, but the exact dates and location have not been officially announced by FIFA yet.\n"
     ]
    }
   ],
   "source": [
    "### Providing New Information in the Prompt\n",
    "\n",
    "prompt = \"\"\"\n",
    "Who won the 2023 Women's World Cup?\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20d8d05f-a195-4422-a642-e5171981acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Spain won the 2023 Women's World Cup by defeating England 1-0 in the final.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "The 2023 FIFA Women's World Cup (MƒÅori: Ipu Wahine o te Ao FIFA i 2023)[1] was the ninth edition of the FIFA Women's World Cup, the quadrennial international women's football championship contested by women's national teams and organised by FIFA. The tournament, which took place from 20 July to 20 August 2023, was jointly hosted by Australia and New Zealand.[2][3][4] It was the first FIFA Women's World Cup with more than one host nation, as well as the first World Cup to be held across multiple confederations, as Australia is in the Asian confederation, while New Zealand is in the Oceanian confederation. It was also the first Women's World Cup to be held in the Southern Hemisphere.[5]\n",
    "This tournament was the first to feature an expanded format of 32 teams from the previous 24, replicating the format used for the men's World Cup from 1998 to 2022.[2] The opening match was won by co-host New Zealand, beating Norway at Eden Park in Auckland on 20 July 2023 and achieving their first Women's World Cup victory.[6]\n",
    "Spain were crowned champions after defeating reigning European champions England 1‚Äì0 in the final. It was the first time a European nation had won the Women's World Cup since 2007 and Spain's first title, although their victory was marred by the Rubiales affair.[7][8][9] Spain became the second nation to win both the women's and men's World Cup since Germany in the 2003 edition.[10] In addition, they became the first nation to concurrently hold the FIFA women's U-17, U-20, and senior World Cups.[11] Sweden would claim their fourth bronze medal at the Women's World Cup while co-host Australia achieved their best placing yet, finishing fourth.[12] Japanese player Hinata Miyazawa won the Golden Boot scoring five goals throughout the tournament. Spanish player Aitana Bonmat√≠ was voted the tournament's best player, winning the Golden Ball, whilst Bonmat√≠'s teammate Salma Paralluelo was awarded the Young Player Award. England goalkeeper Mary Earps won the Golden Glove, awarded to the best-performing goalkeeper of the tournament.\n",
    "Of the eight teams making their first appearance, Morocco were the only one to advance to the round of 16 (where they lost to France; coincidentally, the result of this fixture was similar to the men's World Cup in Qatar, where France defeated Morocco in the semi-final). The United States were the two-time defending champions,[13] but were eliminated in the round of 16 by Sweden, the first time the team had not made the semi-finals at the tournament, and the first time the defending champions failed to progress to the quarter-finals.[14]\n",
    "Australia's team, nicknamed the Matildas, performed better than expected, and the event saw many Australians unite to support them.[15][16][17] The Matildas, who beat France to make the semi-finals for the first time, saw record numbers of fans watching their games, their 3‚Äì1 loss to England becoming the most watched television broadcast in Australian history, with an average viewership of 7.13 million and a peak viewership of 11.15 million viewers.[18]\n",
    "It was the most attended edition of the competition ever held.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following context, who won the 2023 Women's World cup?\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a6da388-5966-43ea-a1de-4eba4832c529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down:\n",
      "\n",
      "* Two people with cars, each seating 5 people: That's a total of 10 people who can be transported in cars.\n",
      "* The remaining 5 people need transportation. Two people have motorcycles, each seating 2 people: That's a total of 4 people who can be transported on motorcycles.\n",
      "\n",
      "Add the number of people who can be transported by car (10) and motorcycle (4): We have a total of 14 people who can be transported. However, there are 15 people in your group, which means one person will not get to ride with you.\n"
     ]
    }
   ],
   "source": [
    "### Chain-of-thought Prompting\n",
    "## LLMs can perform better at reasoning and logic problems if you ask them to break the problem down into smaller steps.\n",
    "## This is known as **chain-of-thought** prompting.\n",
    "\n",
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f17faf67-df74-4a5e-a723-31a0ac5c0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step!\n",
      "\n",
      "We have:\n",
      "\n",
      "* 15 people wanting to go to a restaurant\n",
      "* 2 people with cars, each seating 5 people (total capacity: 10)\n",
      "* 2 people with motorcycles, each seating 2 people (total capacity: 4)\n",
      "\n",
      "To determine if we can all get to the restaurant by car or motorcycle, let's calculate the total number of people who can be transported:\n",
      "\n",
      "1. Cars: 10 people\n",
      "2. Motorcycles: 4 people\n",
      "\n",
      "Total capacity: 14 people\n",
      "\n",
      "Unfortunately, this is less than the total number of people (15). So, we have a shortage of 1 person.\n",
      "\n",
      "Since each car and motorcycle has a fixed capacity, we cannot add more people to the existing vehicles. We need to find an alternative or add more transportation options!\n",
      "\n",
      "One possible solution could be:\n",
      "\n",
      "* The two people with cars take 5 people each (total: 10)\n",
      "* The two people with motorcycles take 1 person each (total: 2)\n",
      "\n",
      "This way, we can transport a total of 12 people. However, this still leaves us short 3 people from the original group.\n",
      "\n",
      "To accommodate everyone, we might need to consider additional transportation options or adjust our plan accordingly!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step and solve arithmetically.\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11a3304a-2ae7-4ab2-a7e3-78871d25e673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step:\n",
      "\n",
      "1. We have 15 people and only 2 cars that can seat 5 people each, so we can fit at most 10 people (2 cars x 5 people per car) initially.\n",
      "\n",
      "Intermediate result: 10 people can be accommodated in the 2 cars, leaving 5 people unaccounted for.\n",
      "\n",
      "2. Since 2 of us have motorcycles that can fit 2 people each, we can use these to transport 4 people (2 motorcycles x 2 people per motorcycle). This leaves 1 person still unaccounted for.\n",
      "\n",
      "Intermediate result: The 10 people initially accommodated in the cars are now all taken care of, leaving only 1 person remaining.\n",
      "\n",
      "3. Since only 1 person is left and we don't have any more vehicles to accommodate them, it's not possible to get everyone to the restaurant by car or motorcycle.\n",
      "\n",
      "Final answer: No, not all 15 people can get to the restaurant by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "Explain each intermediate step.\n",
    "Only when you are done with all your steps,\n",
    "provide the final answer to the question in one line based on your intermediate steps.\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7812d68b-8f83-4645-ab39-f753a607c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Answer:** No\n",
      "\n",
      "Let's break it down:\n",
      "\n",
      "1. Two people have cars, and each car can seat 5 people.\n",
      "\n",
      "Cars: 2 x 5 = 10 people can be transported by cars\n",
      "\n",
      "2. The remaining 15 - 10 = 5 people need to be accommodated on motorcycles or in cars.\n",
      "\n",
      "3. Two people have motorcycles, and each motorcycle can fit 2 people.\n",
      "\n",
      "Motorcycles: 2 x 2 = 4 people can be transported by motorcycles\n",
      "\n",
      "4. Add the number of people that can be transported by cars and motorcycles:\n",
      "\n",
      "Cars + Motorcycles = 10 (from step 1) + 4 (from step 3) = 14\n",
      "\n",
      "5. We still have 15 - 14 = 1 person left who cannot be accommodated.\n",
      "\n",
      "Since one person cannot be accommodated, the answer is **No**.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Calculate arithmetically each intermediate step.\n",
    "Provide the answer to the question as a single yes or no first based on the above calculation.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88dfcbb8-1ae7-4e0d-ad18-cf42ad073dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, I can analyze the relationships between Jeff, Tommy, Eddy, and the concept of neighborhood.\n",
      "\n",
      "From the first statement, we know that:\n",
      "\n",
      "1. Jeff and Tommy are neighbors.\n",
      "\n",
      "This implies that they live in adjacent houses or within a certain proximity.\n",
      "\n",
      "The second statement informs us that:\n",
      "\n",
      "2. Tommy and Eddy are not neighbors.\n",
      "\n",
      "This means that Tommy's house is not next to Eddy's house.\n",
      "\n",
      "Now, let's examine the query: \"Are Jeff and Eddy neighbors?\"\n",
      "\n",
      "To answer this question, we need to consider whether there is any direct relationship between Jeff and Eddy regarding their proximity or adjacency. Since we don't have any information linking Jeff and Eddy as neighbors, we can conclude that:\n",
      "\n",
      "**Jeff and Eddy are not neighbors.**\n",
      "\n",
      "My reasoning is based on the fact that there is no direct evidence suggesting a neighborhood connection between Jeff and Eddy. The only relevant information pertains to Jeff's relationship with Tommy (neighbors) and Tommy's lack of a neighborhood connection with Eddy.\n",
      "\n",
      "Therefore, I can confidently answer: **No**, Jeff and Eddy are not neighbors.\n"
     ]
    }
   ],
   "source": [
    "### Reasoning ###\n",
    "\n",
    "context = \"\"\"\n",
    "Jeff and Tommy are neighbors\n",
    "\n",
    "Tommy and Eddy are not neighbors\n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "Are Jeff and Eddy neighbors?\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given this context: ```{context}```,\n",
    "\n",
    "and the following query:\n",
    "```{query}```\n",
    "\n",
    "Please answer the questions in the query and explain your reasoning.\n",
    "If there is not enough informaton to answer, please say\n",
    "\"I do not have enough information to answer this questions.\"\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa6f20df-1a46-4829-abe1-b45f95ca7a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step:\n",
      "\n",
      "1. We know Jeff and Tommy are neighbors.\n",
      "2. We know Tommy and Eddy are not neighbors.\n",
      "3. We know Jeff's house is between Tommy's and Eddy's house.\n",
      "\n",
      "From statement 1, we can conclude that Tommy lives next to Jeff (or vice versa).\n",
      "From statement 2, we know Eddy does not live next to Tommy.\n",
      "From statement 3, we know Jeff's house is sandwiched between Tommy's and Eddy's houses. This implies that Tommy's house must be on one side of Jeff's house, and Eddy's house must be on the other side.\n",
      "\n",
      "Now, let's ask ourselves: Are Jeff and Eddy neighbors?\n",
      "\n",
      "Since Jeff's house is between Tommy's and Eddy's, it's possible for Eddy to live next to Jeff (or vice versa). We don't have any information that explicitly says they're not neighbors. In fact, the only thing we know is that Tommy doesn't live next to Eddy.\n",
      "\n",
      "Therefore, I'd say: **Yes**, Jeff and Eddy are neighbors.\n",
      "\n",
      "If someone asked me about Tommy's relationship with Eddy, though, I'd say: \"I do not have enough information to answer this question.\" We can't determine if they're neighbors or not based on the given context.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Jeff and Tommy are neighbors\n",
    "\n",
    "Tommy and Eddy are not neighbors\n",
    "\n",
    "Jeff house is between Tommy's and Eddy's house.\n",
    "\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "Are Jeff and Eddy neighbors?\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given this context: ```{context}```,\n",
    "\n",
    "and the following query:\n",
    "```{query}```\n",
    "\n",
    "Please answer the questions in the query and explain your reasoning.\n",
    "If there is not enough informaton to answer, please say\n",
    "\"I do not have enough information to answer this questions.\"\n",
    "\"\"\"\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b14313-a026-4b36-b4e2-d6b567f25c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
